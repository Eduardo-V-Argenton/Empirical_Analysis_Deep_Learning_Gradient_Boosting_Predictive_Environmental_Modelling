{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdde53ef19ea8f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T19:05:40.700276Z",
     "start_time": "2025-06-05T19:05:40.697643Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from darts import TimeSeries\n",
    "from darts.models import TSMixerModel\n",
    "from darts.metrics import wmape,mse, mae, smape,r2_score,rmse,mae\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "import torch\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from pytorch_lightning.callbacks import Callback, EarlyStopping\n",
    "import optuna\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c332b10d8be5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T19:05:40.762080Z",
     "start_time": "2025-06-05T19:05:40.750357Z"
    }
   },
   "outputs": [],
   "source": [
    "target_columns = [\n",
    "    'Temperature','Precipitation_accumulated','Humidity', \n",
    "    'Soil_Moisture', 'Soil_Temperature'\n",
    "]\n",
    "\n",
    "DATA_FILE_PATH = \"../data/ground_station_clean.csv\"\n",
    "df = pd.read_csv(DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f7732f60d8511",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T19:05:40.802223Z",
     "start_time": "2025-06-05T19:05:40.800123Z"
    }
   },
   "outputs": [],
   "source": [
    "encoders = {\n",
    "    \"cyclic\": {\n",
    "        \"past\": [\"month\", \"dayofyear\", \"day\", \"hour\", \"minute\"],\n",
    "        \"future\": [\"month\", \"dayofyear\", \"day\", \"hour\", \"minute\"]\n",
    "    },\n",
    "    \"transformer\": Scaler(),\n",
    "    \"datetime_attribute\": {\n",
    "        \"past\": [\"year\"],\n",
    "        \"future\": [\"year\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0a527e2e2a0a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T19:05:40.852433Z",
     "start_time": "2025-06-05T19:05:40.850235Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchedPruningCallback(PyTorchLightningPruningCallback, Callback):\n",
    "    pass\n",
    "\n",
    "# Create directory to save iteration results\n",
    "results_output_dir = \"optuna_iteration_metrics\"\n",
    "os.makedirs(results_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36495cef7ef43264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T19:05:40.906974Z",
     "start_time": "2025-06-05T19:05:40.899470Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    input_chunk_length = trial.suggest_int(\"input_chunk_length\", 24, 168, step=12)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256, step=8)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.05)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 64, 256, step=8)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 5e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "\n",
    "    ff_size = trial.suggest_int('ff_size', hidden_size, hidden_size * 4)\n",
    "    num_blocks = trial.suggest_int('num_blocks', 1, 4)\n",
    "    activation = trial.suggest_categorical('activation', ['ReLU', 'GELU'])\n",
    "    norm_type = trial.suggest_categorical('norm_type', ['LayerNormNoBias', 'LayerNorm', 'TimeBatchNorm2d'])\n",
    "    normalize_before = trial.suggest_categorical('normalize_before', [True, False])\n",
    "\n",
    "    early_stopper = EarlyStopping(\"val_loss\", min_delta=0.0005, patience=15, verbose=True)\n",
    "    prunner = PatchedPruningCallback(trial, monitor=\"val_loss\")\n",
    "    pl_trainer_kwargs = {\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"callbacks\": [early_stopper, prunner],\n",
    "    }\n",
    "\n",
    "    n_splits = 5\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    overall_smape_list = []\n",
    "    metrics_per_fold = []\n",
    "    fold = 0\n",
    "\n",
    "    for train_index, val_index in tscv.split(df):\n",
    "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "        train_df_fold = df.iloc[train_index]\n",
    "        val_df_fold = df.iloc[val_index]\n",
    "        train_fold = TimeSeries.from_dataframe(train_df_fold, time_col=\"Timestamp\", value_cols=target_columns, freq='1h')\n",
    "        val_fold = TimeSeries.from_dataframe(val_df_fold, time_col=\"Timestamp\", value_cols=target_columns, freq='1h')\n",
    "\n",
    "        scaler = Scaler()\n",
    "        scaler = scaler.fit(train_fold)\n",
    "        train_scaled = scaler.transform(train_fold)\n",
    "        val_scaled = scaler.transform(val_fold)\n",
    "\n",
    "        opt_kwargs = {\"lr\": lr, \"weight_decay\": weight_decay}\n",
    "\n",
    "        model = TSMixerModel(\n",
    "            model_name=f\"model_{fold+1}\",\n",
    "            work_dir=\"/home/eduardo/Documentos/Water-Cycle-Neural-Network/darts_logs\",\n",
    "            input_chunk_length=input_chunk_length,\n",
    "            output_chunk_length=24,\n",
    "            hidden_size=hidden_size,\n",
    "            n_epochs=30,\n",
    "            batch_size=batch_size,\n",
    "            dropout=dropout,\n",
    "            ff_size=ff_size,\n",
    "            num_blocks=num_blocks,\n",
    "            activation=activation,\n",
    "            norm_type=norm_type,\n",
    "            normalize_before=normalize_before,\n",
    "            pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "            loss_fn=torch.nn.L1Loss(),\n",
    "            optimizer_cls=torch.optim.Adam,\n",
    "            lr_scheduler_cls=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            lr_scheduler_kwargs={\"mode\": \"min\", \"factor\": 0.3, \"patience\": 7, \"min_lr\": 1e-7},\n",
    "            save_checkpoints=True,\n",
    "            force_reset=True,\n",
    "            random_state=42,\n",
    "            optimizer_kwargs=opt_kwargs,\n",
    "        )\n",
    "        model.fit(\n",
    "            series=train_scaled,\n",
    "            val_series=val_scaled,\n",
    "            verbose=False,\n",
    "            dataloader_kwargs={\"num_workers\": 11},\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            loaded_model = model.load_from_checkpoint(f\"/home/eduardo/Documentos/Water-Cycle-Neural-Network/darts_logs/model_{fold+1}\", best=True)\n",
    "            print(f\"Model loaded from checkpoint for trial {trial.number}, fold {fold}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Checkpoint not found. Using the in-memory trained model.\")\n",
    "            loaded_model = model\n",
    "\n",
    "        forecasts = loaded_model.historical_forecasts(\n",
    "            val_scaled,\n",
    "            forecast_horizon=24,\n",
    "            stride=1,\n",
    "            retrain=False,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "\n",
    "        forecasts_t = scaler.inverse_transform(forecasts)\n",
    "        s = scaler.inverse_transform(val_scaled)\n",
    "\n",
    "        overall_smape_val = smape(s, forecasts_t)\n",
    "        print(f\"SMAPE fold {fold}: {overall_smape_val}\")\n",
    "        overall_smape_list.append(overall_smape_val)\n",
    "        \n",
    "        metrics = {}\n",
    "        for target in target_columns:\n",
    "            metrics[target] = {\n",
    "                'MSE': mse(s[target], forecasts_t[target]),\n",
    "                'RMSE': rmse(s[target], forecasts_t[target]),\n",
    "                'MAE': mae(s[target], forecasts_t[target]),\n",
    "                'R2': r2_score(s[target], forecasts_t[target]),\n",
    "                'SMAPE': smape(s[target], forecasts_t[target]),\n",
    "                'WAPE': wmape(s[target], forecasts_t[target]),\n",
    "            }\n",
    "        metrics_per_fold.append(metrics)\n",
    "        fold += 1\n",
    "\n",
    "    mean_smape = np.mean(overall_smape_list)\n",
    "    print(f\"MÃ©dia dos SMAPE nos folds: {mean_smape}\")\n",
    "\n",
    "    trial_dict = {\n",
    "        \"trial_number\": trial.number,\n",
    "        \"fold_smape\": overall_smape_list,\n",
    "        \"mean_smape\": mean_smape,\n",
    "        \"hyperparameters\": trial.params,\n",
    "        \"metrics_per_fold\": metrics_per_fold\n",
    "    }\n",
    "    json_path = os.path.join(results_output_dir, f\"trial_{trial.number}.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(trial_dict, f, indent=4)\n",
    "    print(f\"Results of trial {trial.number} saved in {json_path}\")\n",
    "    return mean_smape if not np.isnan(mean_smape) else float(\"inf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36084313658a946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T19:05:40.951388Z",
     "start_time": "2025-06-05T19:05:40.949280Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_callback(study, trial):\n",
    "    print(f\"Current value: {trial.value}, Current params: {trial.params}\")\n",
    "    print(f\"Best value: {study.best_value}, Best params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016d2f3570996c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T19:07:50.838576Z",
     "start_time": "2025-06-05T19:05:40.997387Z"
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "num_hyperparams = 11\n",
    "n_trials = 4 * num_hyperparams\n",
    "study.optimize(objective, n_trials=n_trials, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bce4e436d5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best MASE Value (Minimum): {study.best_value}\")\n",
    "print(f\"Best Parameters: {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de912349146bf8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dict = {\n",
    "    \"best_value\": study.best_value,\n",
    "    \"best_params\": study.best_params,\n",
    "}\n",
    "json_path = os.path.join(results_output_dir, \"best_trial.json\")\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(best_dict, f, indent=4)\n",
    "print(f\"Best results saved in {json_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
