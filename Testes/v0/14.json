{
  "hidden_size": 256,
  "layers": 2,
  "learning_rate": 0.001,
  "batch_size": 64,
  "Loss": "HuberLoss",
  "Optimizer": "Adam",
  "bideretional": true,
  "dropout": 0.2,
  "MSE": 4.075,
  "MAE": 1.6154,
  "MAPE": 0.0861,
  "R2": 0.9144,
  "weight_decay": 1e-4,
  "scheduler": {
    "algorithm": "ReduceLROnPlateau",
    "patience": 10,
    "factor": 0.5,
    "min_lr": 1e-6
  },
  "epochs": {
    "size": 200,
    "delta_losses": 5,
    "patience": {
      "epochs": 20,
      "method": "avg loss"
    },
    "losses": [
      0.0621, 0.0432, 0.0427, 0.0425, 0.0418, 0.0415, 0.0411, 0.0411, 0.0412,
      0.0409, 0.0408, 0.0407, 0.0408, 0.0407, 0.0409, 0.0409, 0.0404, 0.0403,
      0.0403, 0.0403, 0.0402, 0.0403, 0.0402, 0.0403, 0.0403, 0.0403, 0.0403,
      0.0401, 0.04, 0.0401, 0.0401, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.0399,
      0.0399, 0.0399
    ],
    "obs": "Add camada de atention"
  }
}
