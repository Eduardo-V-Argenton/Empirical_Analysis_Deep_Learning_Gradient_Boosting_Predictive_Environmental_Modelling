# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YboxYnkK8LYrLuKMwbqPzQZlZHWiwhVI
"""

# %%
import pandas as pd
import numpy as np
from scipy.stats import weibull_min, vonmises, gamma
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from plotnine import ggplot, aes, geom_line, labs, theme_minimal, theme

# Configurações iniciais
np.random.seed(42)
start_date = '2025-01-01'
end_date = '2025-12-31'
freq = 'h'
datetime_index = pd.date_range(start=start_date, end=end_date, freq=freq)

# Parâmetros baseados em padrões do litoral paulista [[3]][[8]]
TEMP_MEAN = 22  # °C
TEMP_AMP_ANNUAL = 8  # Variação anual
TEMP_AMP_DAILY = 5   # Variação diária

HUMIDITY_BASE = 80   # % (média anual)
PRECIP_RATE = 0.2    # Probabilidade horária de chuva (ajustado sazonalmente)

WIND_SPEED_SHAPE = 2.0  # Forma da distribuição de Weibull
WIND_SPEED_SCALE = 8.0  # Escala (km/h)

# Funções auxiliares
def generate_temperature(index):
    """Gera temperatura com ciclos diurno e sazonal [[8]]"""
    times = np.arange(len(index))
    annual_cycle = TEMP_AMP_ANNUAL * np.sin(2 * np.pi * (times/8760))
    daily_cycle = TEMP_AMP_DAILY * np.sin(2 * np.pi * (times/24))
    noise = np.random.normal(0, 2, len(index))
    return TEMP_MEAN + annual_cycle + daily_cycle + noise

def generate_humidity(temp, precip):
    """Umidade relativa baseada em temperatura e precipitação [[1]]"""
    base = HUMIDITY_BASE - 0.5 * temp  # Relação inversa com temperatura
    rain_effect = np.where(precip > 0, 15, 0)  # Aumento de 15% durante chuva
    return np.clip(base + rain_effect + np.random.normal(0, 5, len(temp)), 30, 100)

def generate_wind():
    """Velocidade e direção do vento (costa brasileira: predominância de E/SE) [[4]]"""
    speed = weibull_min.rvs(WIND_SPEED_SHAPE, scale=WIND_SPEED_SCALE, size=len(datetime_index))
    direction = vonmises.rvs(loc=np.radians(120), kappa=2, size=len(datetime_index))  # 120° = ESE
    return np.clip(speed, 0, 50), np.degrees(direction) % 360

def generate_precipitation(index):
    """Precipitação com padrão sazonal (mais chuva no verão) [[6]]"""
    monthly_probs = index.month.map({
        1:0.15,2:0.2,3:0.25,4:0.18,5:0.12,6:0.08,
        7:0.05,8:0.06,9:0.1,10:0.15,11:0.2,12:0.25
    })
    precip = np.zeros(len(index))
    for i, prob in enumerate(monthly_probs):
        if np.random.rand() < prob:
            precip[i] = gamma.rvs(2, scale=5)  # Chuva intensa em mm/h
    return np.clip(precip, 0, 150)

def generate_soil_vars(temp, precip):
    """Umidade e temperatura do solo [[2]]"""
    soil_moisture = np.zeros(len(temp))
    soil_temp = np.zeros(len(temp))
    for t in range(1, len(temp)):
        # Dinâmica de umidade do solo
        evap = 0.1 * temp[t] if temp[t] > 15 else 0.05 * temp[t]
        soil_moisture[t] = soil_moisture[t-1] + precip[t] - evap
        soil_moisture[t] = np.clip(soil_moisture[t], 10, 100)  # % de saturação

        # Temperatura do solo (resposta amortecida)
        soil_temp[t] = 0.8 * soil_temp[t-1] + 0.2 * temp[t]
    return soil_moisture, soil_temp

# Geração dos dados
temp = generate_temperature(datetime_index)
precip = generate_precipitation(datetime_index)
humid = generate_humidity(temp, precip)
wind_speed, wind_dir = generate_wind()
soil_moist, soil_temp = generate_soil_vars(temp, precip)

# Criação do DataFrame
data = pd.DataFrame({
    'datetime': datetime_index,
    'temperature': np.round(temp, 1),
    'humidity': np.round(humid).astype(int),
    'wind_speed_kmh': np.round(wind_speed, 1),
    'wind_direction_deg': np.round(wind_dir).astype(int),
    'precipitation_mm': np.round(precip, 1),
    'soil_moisture_pct': np.round(soil_moist).astype(int),
    'soil_temperature': np.round(soil_temp, 1)
})


data.head()

#data.set_index('datetime', inplace=True)
#data.head()

print(data.isnull().sum())  # Verifique colunas com NaN

# Preencha valores ausentes (usando forward-fill e backward-fill)
data.fillna(method='ffill', inplace=True)  # Preenche com o valor anterior
data.fillna(method='bfill', inplace=True)  # Preenche com o próximo valor (caso o primeiro valor seja NaN)

import pandas as pd
import numpy as np
from sklearn.preprocessing import (
    StandardScaler,
    MinMaxScaler,
    RobustScaler,
    FunctionTransformer
)
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Transformação para direção do vento (circular)
def transform_wind_direction(X):
    wind_dir_rad = np.deg2rad(X)  # Converte graus para radianos
    return np.column_stack([np.sin(wind_dir_rad), np.cos(wind_dir_rad)])

wind_dir_transformer = Pipeline([
    ('wind_dir', FunctionTransformer(transform_wind_direction)),
    ('scaler', StandardScaler())
])

# Configurar ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('temperature', StandardScaler(), ['temperature']),
        ('humidity', MinMaxScaler(feature_range=(0, 1)), ['humidity']),
        ('wind_speed', RobustScaler(), ['wind_speed_kmh']),
        ('wind_direction', wind_dir_transformer, ['wind_direction_deg']),
        ('precipitation', RobustScaler(), ['precipitation_mm']),
        ('soil_moisture', MinMaxScaler(feature_range=(0, 1)), ['soil_moisture_pct']),
        ('soil_temp', StandardScaler(), ['soil_temperature'])
    ],
    remainder='passthrough'
)

train_size = int(0.8 * len(data))  # 80% para treino
train_data = data.iloc[:train_size]
test_data = data.iloc[train_size:]

X_train = train_data.drop(columns=['datetime'])
y_train = train_data['temperature']
preprocessor.fit(X_train)

X_train_scaled = preprocessor.transform(X_train)
X_test_scaled = preprocessor.transform(test_data.drop(columns=['datetime']))
y_test = test_data['temperature']

X_train_scaled.shape

def create_sequences(data, targets, seq_length):
    sequences = []
    target_seq = []
    for i in range(len(data) - seq_length):
        sequences.append(data[i:i+seq_length])
        target_seq.append(targets[i+seq_length])
    return torch.FloatTensor(sequences), torch.FloatTensor(target_seq)

seq_length = 24
X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, seq_length)
X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, seq_length)

print(f"Formato do treino: {X_train_seq.shape} {y_train_seq.shape}")  # (amostras, look_back, características)
print(f"Formato do teste: {X_test_seq.shape} {y_test_seq.shape}")

# 3. Definir modelo LSTM
class LSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_size, 1)

    def forward(self, x):
        x, (hn, cn) = self.lstm(x)  # hn: hidden state, cn: cell state
        x = self.linear(x[:, -1, :])  # Pegar apenas a última saída
        return x

model = LSTM(input_size=8, hidden_size=64, num_layers=1)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

X_train = torch.FloatTensor(X_train_seq)
y_train = torch.FloatTensor(y_train_seq)  # or LongTensor for classification

dataset = torch.utils.data.TensorDataset(X_train, y_train)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)

# %%
# 4. Treinamento
epochs = 100
losses = []

for epoch in range(epochs):
  for batch_x, batch_y in dataloader:
    model.train()
    optimizer.zero_grad()
    outputs = model(batch_x)  # Add dimensão de features
    loss = criterion(outputs.squeeze(), batch_y)

    loss.backward()
    optimizer.step()

  losses.append(loss.item())
  if epoch % 5 == 0:
    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')

# %%
from plotnine import ggplot, aes, geom_line, geom_point, labs, theme_minimal, scale_color_manual, facet_wrap

# 1. Plotar a perda de treinamento
loss_df = pd.DataFrame({'Epoch': range(epochs), 'Loss': losses})
(ggplot(loss_df, aes(x='Epoch', y='Loss')) +
 geom_line(color='#2c7bb6') +
 labs(title='Training Loss Over Epochs', x='Epoch', y='MSE Loss') +
 theme_minimal())

# 2. Preparar dados para visualização das previsões
def prepare_plot_data(original_data, predictions, is_train=True):
    offset = seq_length if is_train else 0
    dates = original_data['datetime'].iloc[offset:offset+len(predictions)]
    return pd.DataFrame({
        'datetime': dates,
        'Actual': original_data['temperature'].iloc[offset:offset+len(predictions)].values,
        'Predicted': predictions
    })

# Gerar previsões
model.eval()
with torch.no_grad():
    train_pred = model(X_train_seq).numpy().flatten()
    test_pred = model(X_test_seq).numpy().flatten()

# Criar DataFrames
train_plot_df = prepare_plot_data(train_data, train_pred, is_train=True)
test_plot_df = prepare_plot_data(test_data, test_pred, is_train=False)

# Combinar dados para plotagem
full_plot_df = pd.concat([train_plot_df.assign(Set='Train'),
                         test_plot_df.assign(Set='Test')])

# 3. Plotar resultados completos
plot = (ggplot(full_plot_df, aes(x='datetime')) +
 geom_line(aes(y='Actual', color='Actual'), size=0.8) +
 geom_line(aes(y='Predicted', color='Predicted'), size=0.8, linetype='dashed') +
 facet_wrap('~Set', scales='free_x', ncol=1) +
 scale_color_manual(values={'Actual': '#2c7bb6', 'Predicted': '#d7191c'}) +
 labs(title='Temperature Predictions vs Actual',
      x='Date',
      y='Temperature (°C)',
      color='Legend') +
 theme_minimal() +
 theme(figure_size=(12, 8),
       subplots_adjust={'hspace': 0.2}))

plot.save('temperature_predictions_vs_actual.png')

# 4. Plotar detalhe de uma semana de previsões
week_slice = test_plot_df.iloc[:168]  # Primeira semana de dados de teste

plot = (ggplot(week_slice, aes(x='datetime')) +
 geom_line(aes(y='Actual'), color='#2c7bb6', size=1) +
 geom_line(aes(y='Predicted'), color='#d7191c', size=1, linetype='dashed') +
 geom_point(aes(y='Predicted'), color='#d7191c', size=1.5, alpha=0.6) +
 labs(title='Weekly Temperature Prediction Detail',
      x='Date',
      y='Temperature (°C)') +
 theme_minimal() +
 theme(figure_size=(12, 4)))

plot.save('weekly_temperature_prediction_detail.png')
